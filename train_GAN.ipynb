{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, random_split\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import cv2\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv('list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform(image):\n",
    "    #Construct the mask\n",
    "    mask=np.zeros((128,128,3))\n",
    "    mask[48:80,48:80]=1\n",
    "    \n",
    "    image=image/np.amax(image) #All entries of image between 0 and 1\n",
    "    image=rescale(image,0.5,anti_aliasing=True,multichannel=True) # Rescale image to 128*128*3\n",
    "    if (image.shape==mask.shape):\n",
    "        i=np.multiply(image,np.ones(mask.shape)-mask) #Find the unmasked portion\n",
    "        mu=np.zeros((128,128,3))\n",
    "        mu[:,:,0]=np.mean(i[:,:,0]) #Calculate the mean over unmasked portion\n",
    "        mu[:,:,1]=np.mean(i[:,:,1])\n",
    "        mu[:,:,2]=np.mean(i[:,:,2])\n",
    "        im=mu*mask+i #Set the outer pixel of channel to mean value calculated above\n",
    "        gray_img = rgb2gray(mask) \n",
    "        ip=np.dstack((im,gray_img)) #Stack final image with the mask\n",
    "        if (image is not None and ip is not None and mask is not None):\n",
    "            return (image,ip,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacesDataset(Dataset):\n",
    "    def __init__(self,df,img_dir,transform=None):\n",
    "        self.df=df\n",
    "        self.img_dir=img_dir\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        img_path=os.path.join(self.img_dir,'{}'.format(self.df.loc[idx]['image']))\n",
    "        if(img_path is not None):\n",
    "            image=io.imread(img_path)\n",
    "            if (image is not None):\n",
    "                image,tr_image,mask=image_transform(image)\n",
    "                sample={'orig_image':image,'new_image':tr_image,'mask':mask}\n",
    "                if self.transform:\n",
    "                    sample['orig_image']=self.transform(sample['orig_image'])\n",
    "                    sample['new_image']=self.transform(sample['new_image'])\n",
    "                    sample['mask']=self.transform(sample['mask'])\n",
    "        \n",
    "                return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=PlacesDataset(dataframe,img_dir='images/test_256/',transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "num_train=len(dataset)\n",
    "indices=list(range(num_train))\n",
    "test_idx = indices[-210:]\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "test_data = Subset(dataset,test_idx)\n",
    "train_data=Subset(dataset,train_idx)\n",
    "validation_data,test_data=random_split(test_data,(20,190))\n",
    "a=len(train_data)-75000\n",
    "train_phase_1,train_phase_2,train_phase_3,extra=random_split(train_data,(70000,5000,288900,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_p1 = torch.utils.data.DataLoader(train_phase_1,batch_size=100)\n",
    "train_loader_p2 = torch.utils.data.DataLoader(train_phase_2,batch_size=100)\n",
    "train_loader_p3 = torch.utils.data.DataLoader(train_phase_3,batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=4,out_channels=64,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.norm1=nn.BatchNorm2d(num_features=64)\n",
    "        self.conv2=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=2,dilation=1,stride=2,padding=0)\n",
    "        self.norm2=nn.BatchNorm2d(num_features=128)\n",
    "        self.conv3=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=2,dilation=1,stride=2,padding=0)\n",
    "        self.norm3=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv4=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.norm4=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv5=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=2,stride=1,padding=2)\n",
    "        self.norm5=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv6=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=4,stride=1,padding=4)\n",
    "        self.norm6=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv7=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=4,stride=1,padding=4)\n",
    "        self.norm7=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv8=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=8,stride=1,padding=8)\n",
    "        self.norm8=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv9=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=16,stride=1,padding=16)\n",
    "        self.norm9=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv10=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.norm10=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv11=nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=4,dilation=1,stride=2,padding=1)\n",
    "        self.norm11=nn.BatchNorm2d(num_features=128)\n",
    "        self.conv12=nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.norm12=nn.BatchNorm2d(num_features=128)\n",
    "        self.conv13=nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=4,dilation=1,stride=2,padding=1)\n",
    "        self.norm13=nn.BatchNorm2d(num_features=64)\n",
    "        self.conv14=nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.norm14=nn.BatchNorm2d(num_features=32)\n",
    "        self.conv15=nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,dilation=1,stride=1,padding=1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.sigm=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=x.type(torch.cuda.FloatTensor)\n",
    "        out=self.conv1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm1(out)\n",
    "        out=self.conv2(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm2(out)\n",
    "        out=self.conv3(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm3(out)\n",
    "        out=self.conv4(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm4(out)\n",
    "        out=self.conv5(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm5(out)\n",
    "        out=self.conv6(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm6(out)\n",
    "        out=self.conv7(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm7(out)\n",
    "        out=self.conv8(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm8(out)\n",
    "        out=self.conv9(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm9(out)\n",
    "        out=self.conv10(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm10(out)\n",
    "        out=self.conv11(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm11(out)\n",
    "        out=self.conv12(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm12(out)\n",
    "        out=self.conv13(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm13(out)\n",
    "        out=self.conv14(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.norm14(out)\n",
    "        \n",
    "        out=self.conv15(out)\n",
    "        out=self.sigm(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1g=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm1g=nn.BatchNorm2d(num_features=32)\n",
    "        self.conv2g=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm2g=nn.BatchNorm2d(num_features=64)\n",
    "        self.conv3g=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm3g=nn.BatchNorm2d(num_features=128)\n",
    "        self.conv4g=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm4g=nn.BatchNorm2d(num_features=256)\n",
    "        self.conv5g=nn.Conv2d(in_channels=256,out_channels=512,kernel_size=5,dilation=1,stride=2)\n",
    "        \n",
    "        self.conv1l=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm1l=nn.BatchNorm2d(num_features=32)\n",
    "        self.conv2l=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm2l=nn.BatchNorm2d(num_features=64)\n",
    "        self.conv3l=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=5,dilation=1,stride=2)\n",
    "        self.norm3l=nn.BatchNorm2d(num_features=128)\n",
    "        self.conv4l=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=5,dilation=1,stride=2)\n",
    "        \n",
    "        self.fc=nn.Linear(768,1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.sigm=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=x.type(torch.cuda.FloatTensor)\n",
    "        out_g=self.conv1g(x)\n",
    "        out_g=self.relu(out_g)\n",
    "        out_g=self.norm1g(out_g)\n",
    "        out_g=self.conv2g(out_g)\n",
    "        out_g=self.relu(out_g)\n",
    "        out_g=self.norm2g(out_g)\n",
    "        out_g=self.conv3g(out_g)\n",
    "        out_g=self.relu(out_g)\n",
    "        out_g=self.norm3g(out_g)\n",
    "        out_g=self.conv4g(out_g)\n",
    "        out_g=self.relu(out_g)\n",
    "        out_g=self.norm4g(out_g)\n",
    "        out_g=self.conv5g(out_g)\n",
    "        out_g=self.relu(out_g)\n",
    "        out_g=out_g.view(-1,512)\n",
    "        \n",
    "        \n",
    "        y=x[:,:,32:96,32:96]\n",
    "        out_l=self.conv1l(y)\n",
    "        out_l=self.relu(out_l)\n",
    "        out_l=self.norm1l(out_l)\n",
    "        out_l=self.conv2l(out_l)\n",
    "        out_l=self.relu(out_l)\n",
    "        out_l=self.norm2l(out_l)\n",
    "        out_l=self.conv3l(out_l)\n",
    "        out_l=self.relu(out_l)\n",
    "        out_l=self.norm3l(out_l)\n",
    "        out_l=self.conv4l(out_l)\n",
    "        out_l=self.relu(out_l)\n",
    "        out_l=out_l.view(-1,256)\n",
    "        \n",
    "        out=torch.cat((out_g,out_l),1)\n",
    "        out=self.relu(out)\n",
    "        out=self.fc(out)\n",
    "        out=self.sigm(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(generator,validation_loader,g_criterion,epoch,num_epochs,epochs):\n",
    "    running_loss=0\n",
    "    generator.eval()\n",
    "    \n",
    "    home = expanduser('~')\n",
    "    os.mkdir(home+'/Ashwin/Outpainting/images/val/val_{}'.format(epoch+1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,(sample) in enumerate(validation_loader):\n",
    "            i_p=Variable(sample['new_image']).to(device)\n",
    "            i_n=Variable(sample['orig_image']).to(device)\n",
    "            mask=Variable(sample['mask']).to(device)\n",
    "            mask=mask.type(torch.cuda.FloatTensor)\n",
    "            i_n=i_n.type(torch.cuda.FloatTensor)\n",
    "            i_o=generator(i_p)\n",
    "            loss=g_criterion(torch.mul(i_o,mask),torch.mul(i_n,mask))\n",
    "            running_loss+=loss.item()\n",
    "            io.imsave('images/val/val_{}/{}_src.jpg'.format(epoch+1,i),np.transpose(i_o[0,:,:,:].cpu().numpy(), (1, 2, 0)))\n",
    "            io.imsave('images/val/val_{}/{}_dst.jpg'.format(epoch+1,i),np.transpose(i_p[0,0:3,:,:].cpu().numpy(), (1, 2, 0)))\n",
    "            io.imsave('images/val/val_{}/{}_org.jpg'.format(epoch+1,i),np.transpose(i_n[0,:,:,:].cpu().numpy(), (1, 2, 0)))\n",
    "    \n",
    "    print ('Epoch: [{}/{}] | Validation Loss: {}'.format(epoch+1-epochs, num_epochs,running_loss/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(model_gen,model_dis,train_loader,d_criterion,g_criterion,d_optimizer,g_optimizer,epoch,num_epochs,epochs,alpha,rl_d_real,rl_d_fake,rl_gen):\n",
    "    running_loss_d_real=0\n",
    "    running_loss_d_fake=0\n",
    "    running_loss_g=0\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for i,(sample) in enumerate(train_loader):\n",
    "        i_p=Variable(sample['new_image']).to(device)\n",
    "        i_n=Variable(sample['orig_image']).to(device)\n",
    "        i_n=i_n.type(torch.cuda.FloatTensor)\n",
    "        mask=Variable(sample['mask']).to(device)\n",
    "        mask=mask.type(torch.cuda.FloatTensor)\n",
    "        d_optimizer.zero_grad()\n",
    "        i_o=generator(i_p).detach()\n",
    "        i_p=i_p.type(torch.cuda.FloatTensor)\n",
    "        i_p[:,:,48:80,48:80]=0\n",
    "        img=torch.mul(i_o,mask)+i_p[:,0:3,:,:]\n",
    "        \n",
    "        \n",
    "        #Discriminator Training\n",
    "        \n",
    "        d_i_o=discriminator(img)\n",
    "        d_i_n=discriminator(i_n)\n",
    "        loss_real=d_criterion(d_i_n,ones_target(100).to(device))\n",
    "        loss_real.backward()\n",
    "        rl_d_real.append(loss_real.item())\n",
    "        \n",
    "        loss_fake=d_criterion(d_i_o,zeros_target(100).to(device))\n",
    "        loss_fake.backward()\n",
    "        rl_d_fake.append(loss_fake.item())\n",
    "        \n",
    "        d_optimizer.step()\n",
    "        \n",
    "        running_loss_d_real+=loss_real.item()\n",
    "        running_loss_d_fake+=loss_fake.item()\n",
    "        \n",
    "        #Generator training\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        i_p=Variable(sample['new_image']).to(device)\n",
    "        i_n=Variable(sample['orig_image']).to(device)\n",
    "        i_n=i_n.type(torch.cuda.FloatTensor)\n",
    "        mask=Variable(sample['mask']).to(device)\n",
    "        mask=mask.type(torch.cuda.FloatTensor)\n",
    "        i_o=generator(i_p)\n",
    "        i_p=i_p.type(torch.cuda.FloatTensor)\n",
    "        i_p[:,:,48:80,48:80]=0\n",
    "        img=torch.mul(i_o,mask)+i_p[:,0:3,:,:]\n",
    "        d_i_o=discriminator(img).detach()\n",
    "        loss_g=g_criterion(torch.mul(i_o,mask),torch.mul(i_n,mask)) + alpha*d_criterion(d_i_o,zeros_target(100).to(device))\n",
    "        loss_g.backward()\n",
    "        g_optimizer.step()\n",
    "        d_optimizer.step()\n",
    "        running_loss_g+=loss_g.item()\n",
    "        rl_gen.append(loss_g)\n",
    "        if((i+1)%289==0):\n",
    "            print('Epoch: [{}/{}] | Step: [{}/{}] | Gen_Loss: {} | Dis_Loss_real: {} | Dis_Loss_fake: {}'.format(epoch+1-epochs, num_epochs, int((i+1)/289),10 , round(running_loss_g/289,4), round(running_loss_d_real/289,6), round(running_loss_d_fake/289,6)))\n",
    "            running_loss_g=0\n",
    "            running_loss_d=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "generator=generator.to(device)\n",
    "g_criterion = nn.MSELoss()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=Discriminator()\n",
    "discriminator=discriminator.to(device)\n",
    "d_criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (norm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "  (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "  (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
       "  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
       "  (norm9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (norm10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (norm11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (norm12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv13): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (norm13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv14): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (norm14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv15): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('GAN/gen_model_9.pth')\n",
    "generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epochs_gen = checkpoint['epoch']\n",
    "rl_gen = checkpoint['loss']\n",
    "\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1g): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm1g): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2g): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm2g): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3g): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm3g): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4g): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm4g): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5g): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv1l): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm1l): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2l): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm2l): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3l): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (norm3l): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4l): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('GAN/dis_model_9.pth')\n",
    "discriminator.load_state_dict(checkpoint['model_state_dict'])\n",
    "d_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epochs_dis = checkpoint['epoch']\n",
    "rl_dis_real = checkpoint['loss_real']\n",
    "rl_dis_fake = checkpoint['loss_fake']\n",
    "\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3] | Step: [1/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020431 | Dis_Loss_fake: 0.052293\n",
      "Epoch: [1/3] | Step: [2/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020431 | Dis_Loss_fake: 0.052473\n",
      "Epoch: [1/3] | Step: [3/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020435 | Dis_Loss_fake: 0.052493\n",
      "Epoch: [1/3] | Step: [4/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020444 | Dis_Loss_fake: 0.05251\n",
      "Epoch: [1/3] | Step: [5/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020445 | Dis_Loss_fake: 0.052523\n",
      "Epoch: [1/3] | Step: [6/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020526 | Dis_Loss_fake: 0.052761\n",
      "Epoch: [1/3] | Step: [7/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020526 | Dis_Loss_fake: 0.052769\n",
      "Epoch: [1/3] | Step: [8/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020526 | Dis_Loss_fake: 0.052773\n",
      "Epoch: [1/3] | Step: [9/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.020526 | Dis_Loss_fake: 0.052775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/skimage/util/dtype.py:130: UserWarning: Possible precision loss when converting from float32 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/skimage/util/dtype.py:130: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3] | Validation Loss: 0.008324115863069892\n",
      "%---Saving the model---%\n",
      "Epoch: [2/3] | Step: [1/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.00201 | Dis_Loss_fake: 0.000815\n",
      "Epoch: [2/3] | Step: [2/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.002292 | Dis_Loss_fake: 0.000854\n",
      "Epoch: [2/3] | Step: [3/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.00638 | Dis_Loss_fake: 0.013841\n",
      "Epoch: [2/3] | Step: [4/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.006628 | Dis_Loss_fake: 0.013867\n",
      "Epoch: [2/3] | Step: [5/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.006628 | Dis_Loss_fake: 0.014835\n",
      "Epoch: [2/3] | Step: [6/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.006628 | Dis_Loss_fake: 0.015034\n",
      "Epoch: [2/3] | Step: [7/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.006639 | Dis_Loss_fake: 0.015991\n",
      "Epoch: [2/3] | Step: [8/10] | Gen_Loss: 0.0016 | Dis_Loss_real: 0.008943 | Dis_Loss_fake: 0.016661\n"
     ]
    }
   ],
   "source": [
    "epochs=9\n",
    "num_epochs=3\n",
    "#rl_dis_real=[]\n",
    "#rl_dis_fake=[]\n",
    "#rl_gen=[]\n",
    "for epoch in range(epochs,num_epochs+epochs):\n",
    "    train_both(generator,discriminator,train_loader_p3,d_criterion,g_criterion,d_optimizer,g_optimizer,epoch,num_epochs,epochs,0.0004,rl_dis_real,rl_dis_fake,rl_gen)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'loss_real': rl_dis_real,\n",
    "            'loss_fake': rl_dis_real,\n",
    "            },'GAN/dis_model_{}.pth'.format(epoch+1))\n",
    "    torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'loss': rl_gen,\n",
    "            },'GAN/gen_model_{}.pth'.format(epoch+1))\n",
    "    \n",
    "    evaluate(generator,validation_loader,g_criterion,epoch,num_epochs,epochs)\n",
    "    print ('%---Saving the model---%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
