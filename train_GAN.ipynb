{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, random_split\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv('celeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacesDataset(Dataset):\n",
    "    def __init__(self,df,img_dir,transform=None):\n",
    "        self.df=df\n",
    "        self.img_dir=img_dir\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        img_path=os.path.join(self.img_dir,'{}'.format(self.df.loc[idx]['image']))\n",
    "        if(img_path is not None):\n",
    "            mpv=0.4381295423217147\n",
    "            image=Image.open(img_path)\n",
    "            sample={'orig_image':image}\n",
    "            if self.transform:\n",
    "                sample['orig_image']=self.transform(sample['orig_image'])\n",
    "                mask=np.zeros((160,160,3))\n",
    "                mask[60:100,60:100,:]=1\n",
    "                mask=transforms.functional.to_tensor(mask)\n",
    "                mask=mask.type(torch.FloatTensor)\n",
    "                new_image=sample['orig_image']-sample['orig_image']*mask+mpv*mask\n",
    "                sample['new_image']=new_image\n",
    "                sample['mask']=mask\n",
    "            \n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataframe[:200000]\n",
    "validation=dataframe[200000:200059]\n",
    "test=dataframe[200059:202599]\n",
    "validation=validation.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)\n",
    "train_data=PlacesDataset(train,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))\n",
    "validation_data=PlacesDataset(validation,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))\n",
    "test_data=PlacesDataset(test,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=16)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(Concatenate, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(x, dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h, img_w)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.act6 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=2, padding=2)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        self.act7 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=4, padding=4)\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "        self.act8 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=8, padding=8)\n",
    "        self.bn9 = nn.BatchNorm2d(256)\n",
    "        self.act9 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv10 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=16, padding=16)\n",
    "        self.bn10 = nn.BatchNorm2d(256)\n",
    "        self.act10 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv11 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.act11 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv12 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(256)\n",
    "        self.act12 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.deconv13 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn13 = nn.BatchNorm2d(128)\n",
    "        self.act13 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv14 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn14 = nn.BatchNorm2d(128)\n",
    "        self.act14 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.deconv15 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn15 = nn.BatchNorm2d(64)\n",
    "        self.act15 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h, img_w)\n",
    "        self.conv16 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn16 = nn.BatchNorm2d(32)\n",
    "        self.act16 = nn.ReLU()\n",
    "        # input_shape: (None, 32, img_h, img_w)\n",
    "        self.conv17 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.act17 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.bn6(self.act6(self.conv6(x)))\n",
    "        x = self.bn7(self.act7(self.conv7(x)))\n",
    "        x = self.bn8(self.act8(self.conv8(x)))\n",
    "        x = self.bn9(self.act9(self.conv9(x)))\n",
    "        x = self.bn10(self.act10(self.conv10(x)))\n",
    "        x = self.bn11(self.act11(self.conv11(x)))\n",
    "        x = self.bn12(self.act12(self.conv12(x)))\n",
    "        x = self.bn13(self.act13(self.deconv13(x)))\n",
    "        x = self.bn14(self.act14(self.conv14(x)))\n",
    "        x = self.bn15(self.act15(self.deconv15(x)))\n",
    "        x = self.bn16(self.act16(self.conv16(x)))\n",
    "        x = self.act17(self.conv17(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Discriminator(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Global_Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1024,)\n",
    "        self.img_c = input_shape[0]\n",
    "        self.img_h = input_shape[1]\n",
    "        self.img_w = input_shape[2]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h//2, img_w//2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//4, img_w//4)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//8, img_w//8)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//16, img_w//16)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//32, img_w//32)\n",
    "        in_features = 512 * (self.img_h//32) * (self.img_w//32)\n",
    "        self.flatten6 = Flatten()\n",
    "        self.linear6 = nn.Linear(in_features, 1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.act6(self.linear6(self.flatten6(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local_Discriminator(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Local_Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1024,)\n",
    "        self.img_c = input_shape[0]\n",
    "        self.img_h = input_shape[1]\n",
    "        self.img_w = input_shape[2]\n",
    "\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(self.img_c, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h//2, img_w//2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//4, img_w//4)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//8, img_w//8)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//16, img_w//16)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//32, img_w//32)\n",
    "        in_features = 512 * (self.img_h//32) * (self.img_w//32)\n",
    "        self.flatten6 = Flatten()\n",
    "        # input_shape: (None, 512 * img_h//32 * img_w//32)\n",
    "        self.linear6 = nn.Linear(in_features, 1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        # output_shape: (None, 1024)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.act6(self.linear6(self.flatten6(x)))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, local_input_shape, global_input_shape):\n",
    "\n",
    "        super(ContextDiscriminator, self).__init__()\n",
    "        self.input_shape = [local_input_shape, global_input_shape]\n",
    "        self.output_shape = (1,)\n",
    "        self.model_ld = Local_Discriminator(local_input_shape)\n",
    "        self.model_gd = Global_Discriminator(global_input_shape)\n",
    "        # input_shape: [(None, 1024), (None, 1024)]\n",
    "        in_features = self.model_ld.output_shape[-1] + self.model_gd.output_shape[-1]\n",
    "        self.concat1 = Concatenate(dim=-1)\n",
    "        # input_shape: (None, 2048)\n",
    "        self.linear1 = nn.Linear(in_features, 1)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        # output_shape: (None, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ld, x_gd = x\n",
    "        x_ld = self.model_ld(x_ld)\n",
    "        x_gd = self.model_gd(x_gd)\n",
    "        out = self.act1(self.linear1(self.concat1([x_ld, x_gd])))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator,discriminator,train_loader,g_optimizer,g_criterion,d_optimizer,d_criterion,epoch,num_epochs,epochs,rl_dis,rl_gen,alpha):\n",
    "    running_loss_dis=0\n",
    "    running_loss_gen=0\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "    for i,(sample) in enumerate(train_loader):\n",
    "        #### Discriminator Training ####\n",
    "        i_n=Variable(sample['new_image']).to(device)\n",
    "        i_o=Variable(sample['orig_image']).to(device)\n",
    "        mask=Variable(sample['mask']).to(device)\n",
    "        \n",
    "        i_g=generator(i_o).detach()\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_i_g=discriminator((i_g[:,:,32:128,32:128],i_g))\n",
    "        d_i_o=discriminator((i_o[:,:,32:128,32:128],i_o))\n",
    "        \n",
    "        loss_real=d_criterion(d_i_o,ones_target(16).to(device))\n",
    "        loss_fake=d_criterion(d_i_g,zeros_target(16).to(device))\n",
    "        rl_dis.append((loss_fake.item()+loss_real.item())/2)\n",
    "        \n",
    "        loss_total=alpha*(loss_fake+loss_real)/2\n",
    "        loss_total.backward()\n",
    "        d_optimizer.step()\n",
    "        running_loss_dis+=(loss_fake.item()+loss_real.item())/2\n",
    "        \n",
    "        #### Generator Training ####\n",
    "        \n",
    "        i_n=Variable(sample['new_image']).to(device)\n",
    "        i_o=Variable(sample['orig_image']).to(device)\n",
    "        mask=Variable(sample['mask']).to(device)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        i_g=generator(i_n)\n",
    "        d_i_g=discriminator((i_g[:,:,32:128,32:128],i_g))\n",
    "        loss1=g_criterion(torch.mul(i_o,mask),torch.mul(i_g,mask))\n",
    "        loss2=d_criterion(d_i_g,ones_target(16).to(device))\n",
    "        loss=loss1+alpha*loss2\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "        running_loss_gen+=loss.item()\n",
    "        rl_gen.append(loss.item())\n",
    "        if((i+1)%1250==0):\n",
    "            print('Epoch: [{}/{}] | Step: [{}/{}] | Generator_Loss: {} | Discriminator_Loss: {}'.format(epoch+1-epochs, num_epochs, int((i+1)/1250), 10, round(running_loss_gen/1250,6), round((running_loss_dis)/1250,6)))\n",
    "            running_loss_dis=0\n",
    "            running_loss_gen=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(generator,validation_loader,g_optimizer,g_criterion,epoch):\n",
    "    generator.eval()\n",
    "    running_loss=0\n",
    "    os.mkdir('images/val/val_{}'.format(epoch+1))\n",
    "    with torch.no_grad():\n",
    "        for i,(sample) in enumerate(validation_loader):\n",
    "            i_n=Variable(sample['new_image']).to(device)\n",
    "            i_o=Variable(sample['orig_image']).to(device)\n",
    "            mask=Variable(sample['mask']).to(device)\n",
    "            \n",
    "            i_g=generator(i_n)\n",
    "            io.imsave('images/val/val_{}/{}_src.jpg'.format(epoch+1,i+1),np.transpose(i_g[0].cpu().numpy(), (1, 2, 0)))\n",
    "            io.imsave('images/val/val_{}/{}_dst.jpg'.format(epoch+1,i+1),np.transpose(i_n[0].cpu().numpy(), (1, 2, 0)))\n",
    "            io.imsave('images/val/val_{}/{}_orig.jpg'.format(epoch+1,i+1),np.transpose(i_o[0].cpu().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            loss=g_criterion(torch.mul(i_o,mask),torch.mul(i_g,mask))\n",
    "            running_loss+=loss.item()\n",
    "    print ('Validation loss: {}'.format(running_loss/59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "generator=generator.to(device)\n",
    "g_criterion = nn.MSELoss()\n",
    "g_optimizer = torch.optim.Adadelta(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = ContextDiscriminator(local_input_shape=(3, 96, 96), global_input_shape=(3, 160, 160))\n",
    "discriminator=discriminator.to(device)\n",
    "d_criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adadelta(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU()\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act5): ReLU()\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act6): ReLU()\n",
       "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act7): ReLU()\n",
       "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act8): ReLU()\n",
       "  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
       "  (bn9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act9): ReLU()\n",
       "  (conv10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
       "  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act10): ReLU()\n",
       "  (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act11): ReLU()\n",
       "  (conv12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act12): ReLU()\n",
       "  (deconv13): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act13): ReLU()\n",
       "  (conv14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act14): ReLU()\n",
       "  (deconv15): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act15): ReLU()\n",
       "  (conv16): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act16): ReLU()\n",
       "  (conv17): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act17): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('Generator/model_7.pth')\n",
    "generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextDiscriminator(\n",
       "  (model_ld): Local_Discriminator(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act4): ReLU()\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act5): ReLU()\n",
       "    (flatten6): Flatten()\n",
       "    (linear6): Linear(in_features=4608, out_features=1024, bias=True)\n",
       "    (act6): ReLU()\n",
       "  )\n",
       "  (model_gd): Global_Discriminator(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act4): ReLU()\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act5): ReLU()\n",
       "    (flatten6): Flatten()\n",
       "    (linear6): Linear(in_features=12800, out_features=1024, bias=True)\n",
       "    (act6): ReLU()\n",
       "  )\n",
       "  (concat1): Concatenate()\n",
       "  (linear1): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (act1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('Discriminator/model_1.pth')\n",
    "discriminator.load_state_dict(checkpoint['model_state_dict'])\n",
    "d_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/9] | Step: [1/10] | Generator_Loss: 471.999714 | Discriminator_Loss: 0.005964\n",
      "Epoch: [1/9] | Step: [2/10] | Generator_Loss: 461.546918 | Discriminator_Loss: 0.00402\n",
      "Epoch: [1/9] | Step: [3/10] | Generator_Loss: 458.03949 | Discriminator_Loss: 0.002237\n",
      "Epoch: [1/9] | Step: [4/10] | Generator_Loss: 452.027082 | Discriminator_Loss: 0.00269\n",
      "Epoch: [1/9] | Step: [5/10] | Generator_Loss: 438.976389 | Discriminator_Loss: 0.00314\n",
      "Epoch: [1/9] | Step: [6/10] | Generator_Loss: 439.954068 | Discriminator_Loss: 0.001806\n",
      "Epoch: [1/9] | Step: [7/10] | Generator_Loss: 433.943969 | Discriminator_Loss: 0.00159\n",
      "Epoch: [1/9] | Step: [8/10] | Generator_Loss: 426.177525 | Discriminator_Loss: 0.001928\n",
      "Epoch: [1/9] | Step: [9/10] | Generator_Loss: 422.860345 | Discriminator_Loss: 0.001216\n",
      "Epoch: [1/9] | Step: [10/10] | Generator_Loss: 423.864591 | Discriminator_Loss: 0.001543\n",
      "Validation loss: 0.000400650693829787\n",
      "%---Saving the model---%\n",
      "Epoch: [2/9] | Step: [1/10] | Generator_Loss: 416.340325 | Discriminator_Loss: 0.001656\n",
      "Epoch: [2/9] | Step: [2/10] | Generator_Loss: 410.008113 | Discriminator_Loss: 0.002141\n"
     ]
    }
   ],
   "source": [
    "num_epochs=9\n",
    "epochs=1\n",
    "#rl_dis=[]\n",
    "#rl_gen=[]\n",
    "for epoch in range(epochs,num_epochs+epochs):\n",
    "    train_gan(generator,discriminator,train_loader,g_optimizer,g_criterion,d_optimizer,d_criterion,epoch,num_epochs,epochs,rl_dis,rl_gen,0.0004)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'loss': rl_dis,\n",
    "            },'GAN/dis_model_{}.pth'.format(epoch+1))\n",
    "    torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'loss': rl_gen,\n",
    "            },'GAN/gen_model_{}.pth'.format(epoch+1))\n",
    "    \n",
    "    evaluate(generator,validation_loader,g_optimizer,g_criterion,epoch)\n",
    "    print ('%---Saving the model---%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.00040967306132049506\n"
     ]
    }
   ],
   "source": [
    "evaluate(generator,validation_loader,g_optimizer,g_criterion,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(rl_gen[:100])),rl_gen[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'loss': rl_dis,\n",
    "            },'GAN/dis_model_{}.pth'.format(epoch+1))\n",
    "torch.save({\n",
    "            'epoch': int(epoch+1),\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'loss': rl_gen,\n",
    "            },'GAN/gen_model_{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.00047475508916373287\n"
     ]
    }
   ],
   "source": [
    "evaluate(generator,validation_loader,g_optimizer,g_criterion,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
