{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, random_split\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv('celeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacesDataset(Dataset):\n",
    "    def __init__(self,df,img_dir,transform=None):\n",
    "        self.df=df\n",
    "        self.img_dir=img_dir\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        img_path=os.path.join(self.img_dir,'{}'.format(self.df.loc[idx]['image']))\n",
    "        if(img_path is not None):\n",
    "            mpv=0.4381295423217147\n",
    "            image=Image.open(img_path)\n",
    "            sample={'orig_image':image}\n",
    "            if self.transform:\n",
    "                sample['orig_image']=self.transform(sample['orig_image'])\n",
    "                mask=np.zeros((160,160,3))\n",
    "                mask[60:100,60:100,:]=1\n",
    "                mask=transforms.functional.to_tensor(mask)\n",
    "                mask=mask.type(torch.FloatTensor)\n",
    "                new_image=sample['orig_image']-sample['orig_image']*mask+mpv*mask\n",
    "                sample['new_image']=new_image\n",
    "                sample['mask']=mask\n",
    "            \n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataframe[:200000]\n",
    "validation=dataframe[200000:200059]\n",
    "test=dataframe[200059:202599]\n",
    "validation=validation.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)\n",
    "train_data=PlacesDataset(train,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))\n",
    "validation_data=PlacesDataset(validation,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))\n",
    "test_data=PlacesDataset(test,img_dir='images/CelebA/',transform=transforms.Compose([transforms.Resize(160),transforms.RandomCrop((160,160)),transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=16)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(Concatenate, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(x, dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h, img_w)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.act6 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=2, padding=2)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        self.act7 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=4, padding=4)\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "        self.act8 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=8, padding=8)\n",
    "        self.bn9 = nn.BatchNorm2d(256)\n",
    "        self.act9 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv10 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=16, padding=16)\n",
    "        self.bn10 = nn.BatchNorm2d(256)\n",
    "        self.act10 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv11 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.act11 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.conv12 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(256)\n",
    "        self.act12 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//4, img_w//4)\n",
    "        self.deconv13 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn13 = nn.BatchNorm2d(128)\n",
    "        self.act13 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.conv14 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn14 = nn.BatchNorm2d(128)\n",
    "        self.act14 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//2, img_w//2)\n",
    "        self.deconv15 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn15 = nn.BatchNorm2d(64)\n",
    "        self.act15 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h, img_w)\n",
    "        self.conv16 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn16 = nn.BatchNorm2d(32)\n",
    "        self.act16 = nn.ReLU()\n",
    "        # input_shape: (None, 32, img_h, img_w)\n",
    "        self.conv17 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.act17 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.bn6(self.act6(self.conv6(x)))\n",
    "        x = self.bn7(self.act7(self.conv7(x)))\n",
    "        x = self.bn8(self.act8(self.conv8(x)))\n",
    "        x = self.bn9(self.act9(self.conv9(x)))\n",
    "        x = self.bn10(self.act10(self.conv10(x)))\n",
    "        x = self.bn11(self.act11(self.conv11(x)))\n",
    "        x = self.bn12(self.act12(self.conv12(x)))\n",
    "        x = self.bn13(self.act13(self.deconv13(x)))\n",
    "        x = self.bn14(self.act14(self.conv14(x)))\n",
    "        x = self.bn15(self.act15(self.deconv15(x)))\n",
    "        x = self.bn16(self.act16(self.conv16(x)))\n",
    "        x = self.act17(self.conv17(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Discriminator(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Global_Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1024,)\n",
    "        self.img_c = input_shape[0]\n",
    "        self.img_h = input_shape[1]\n",
    "        self.img_w = input_shape[2]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h//2, img_w//2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//4, img_w//4)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//8, img_w//8)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//16, img_w//16)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//32, img_w//32)\n",
    "        in_features = 512 * (self.img_h//32) * (self.img_w//32)\n",
    "        self.flatten6 = Flatten()\n",
    "        self.linear6 = nn.Linear(in_features, 1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.act6(self.linear6(self.flatten6(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local_Discriminator(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Local_Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1024,)\n",
    "        self.img_c = input_shape[0]\n",
    "        self.img_h = input_shape[1]\n",
    "        self.img_w = input_shape[2]\n",
    "\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(self.img_c, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # input_shape: (None, 64, img_h//2, img_w//2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # input_shape: (None, 128, img_h//4, img_w//4)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # input_shape: (None, 256, img_h//8, img_w//8)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.act4 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//16, img_w//16)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        # input_shape: (None, 512, img_h//32, img_w//32)\n",
    "        in_features = 512 * (self.img_h//32) * (self.img_w//32)\n",
    "        self.flatten6 = Flatten()\n",
    "        # input_shape: (None, 512 * img_h//32 * img_w//32)\n",
    "        self.linear6 = nn.Linear(in_features, 1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        # output_shape: (None, 1024)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.act1(self.conv1(x)))\n",
    "        x = self.bn2(self.act2(self.conv2(x)))\n",
    "        x = self.bn3(self.act3(self.conv3(x)))\n",
    "        x = self.bn4(self.act4(self.conv4(x)))\n",
    "        x = self.bn5(self.act5(self.conv5(x)))\n",
    "        x = self.act6(self.linear6(self.flatten6(x)))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, local_input_shape, global_input_shape):\n",
    "\n",
    "        super(ContextDiscriminator, self).__init__()\n",
    "        self.input_shape = [local_input_shape, global_input_shape]\n",
    "        self.output_shape = (1,)\n",
    "        self.model_ld = Local_Discriminator(local_input_shape)\n",
    "        self.model_gd = Global_Discriminator(global_input_shape)\n",
    "        # input_shape: [(None, 1024), (None, 1024)]\n",
    "        in_features = self.model_ld.output_shape[-1] + self.model_gd.output_shape[-1]\n",
    "        self.concat1 = Concatenate(dim=-1)\n",
    "        # input_shape: (None, 2048)\n",
    "        self.linear1 = nn.Linear(in_features, 1)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        # output_shape: (None, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ld, x_gd = x\n",
    "        x_ld = self.model_ld(x_ld)\n",
    "        x_gd = self.model_gd(x_gd)\n",
    "        out = self.act1(self.linear1(self.concat1([x_ld, x_gd])))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator,generator,train_loader,d_optimizer,d_criterion,epoch,num_epochs,epochs,rl_d_fake,rl_d_real):\n",
    "    running_loss_real=0\n",
    "    running_loss_fake=0\n",
    "    running_loss_total=0\n",
    "    discriminator.train()\n",
    "    for i,(sample) in enumerate(train_loader):\n",
    "        i_n=Variable(sample['new_image']).to(device)\n",
    "        i_o=Variable(sample['orig_image']).to(device)\n",
    "        mask=Variable(sample['mask']).to(device)\n",
    "        \n",
    "        i_g=generator(i_o).detach()\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_i_g=discriminator((i_g[:,:,32:128,32:128],i_g))\n",
    "        d_i_o=discriminator((i_o[:,:,32:128,32:128],i_o))\n",
    "        \n",
    "        loss_real=d_criterion(d_i_o,ones_target(16).to(device))\n",
    "        rl_d_real.append(loss_real.item())\n",
    "        \n",
    "        loss_fake=d_criterion(d_i_g,zeros_target(16).to(device))\n",
    "        rl_d_fake.append(loss_fake.item())\n",
    "        \n",
    "        loss_total=(loss_fake+loss_real)/2\n",
    "        loss_total.backward()\n",
    "        \n",
    "        \n",
    "        d_optimizer.step()\n",
    "        \n",
    "        running_loss_real+=loss_real.item()\n",
    "        running_loss_fake+=loss_fake.item()\n",
    "        running_loss_total+=loss_total.item()\n",
    "        if((i+1)%1250==0):\n",
    "            print('Epoch: [{}/{}] | Step: [{}/{}] | Loss_Real: {} | Loss_fake: {} | Total loss: {}'.format(epoch+1-epochs, num_epochs, int((i+1)/1250), 10, round(running_loss_real/1250,6), round(running_loss_fake/1250,6), round((running_loss_total)/1250,6)))\n",
    "            running_loss_real=0\n",
    "            running_loss_fake=0\n",
    "            running_loss_total=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "generator=generator.to(device)\n",
    "g_criterion = nn.MSELoss()\n",
    "g_optimizer = torch.optim.Adadelta(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = ContextDiscriminator(local_input_shape=(3, 96, 96), global_input_shape=(3, 160, 160))\n",
    "discriminator=discriminator.to(device)\n",
    "d_criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adadelta(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU()\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act5): ReLU()\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act6): ReLU()\n",
       "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act7): ReLU()\n",
       "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act8): ReLU()\n",
       "  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
       "  (bn9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act9): ReLU()\n",
       "  (conv10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
       "  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act10): ReLU()\n",
       "  (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act11): ReLU()\n",
       "  (conv12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act12): ReLU()\n",
       "  (deconv13): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act13): ReLU()\n",
       "  (conv14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act14): ReLU()\n",
       "  (deconv15): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act15): ReLU()\n",
       "  (conv16): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act16): ReLU()\n",
       "  (conv17): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act17): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('Generator/model_7.pth')\n",
    "generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1] | Step: [1/10] | Loss_Real: 0.009064 | Loss_fake: 0.032091 | Total loss: 0.020577\n",
      "Epoch: [1/1] | Step: [2/10] | Loss_Real: 0.000381 | Loss_fake: 0.000605 | Total loss: 0.000493\n",
      "Epoch: [1/1] | Step: [3/10] | Loss_Real: 5.5e-05 | Loss_fake: 8.1e-05 | Total loss: 6.8e-05\n",
      "Epoch: [1/1] | Step: [4/10] | Loss_Real: 0.000388 | Loss_fake: 0.000457 | Total loss: 0.000423\n",
      "Epoch: [1/1] | Step: [5/10] | Loss_Real: 3e-06 | Loss_fake: 3.9e-05 | Total loss: 2.1e-05\n",
      "Epoch: [1/1] | Step: [6/10] | Loss_Real: 1.9e-05 | Loss_fake: 2.6e-05 | Total loss: 2.3e-05\n",
      "Epoch: [1/1] | Step: [7/10] | Loss_Real: 0.0 | Loss_fake: 0.0 | Total loss: 0.0\n",
      "Epoch: [1/1] | Step: [8/10] | Loss_Real: 0.000136 | Loss_fake: 6.8e-05 | Total loss: 0.000102\n",
      "Epoch: [1/1] | Step: [9/10] | Loss_Real: 1e-06 | Loss_fake: 1e-06 | Total loss: 1e-06\n",
      "Epoch: [1/1] | Step: [10/10] | Loss_Real: 0.0 | Loss_fake: 0.0 | Total loss: 0.0\n",
      "%---Saving the model---%\n"
     ]
    }
   ],
   "source": [
    "num_epochs_dis=1\n",
    "epochs_dis=0\n",
    "rl_d_fake=[]\n",
    "rl_d_real=[]\n",
    "for epoch_dis in range(epochs_dis,num_epochs_dis+epochs_dis):\n",
    "    train_discriminator(discriminator,generator,train_loader,d_optimizer,d_criterion,epoch_dis,num_epochs_dis,epochs_dis,rl_d_fake,rl_d_real)\n",
    "    print ('%---Saving the model---%')\n",
    "    torch.save({\n",
    "            'epoch': int(epoch_dis+1),\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'Fake loss': rl_d_fake,\n",
    "            'Real loss': rl_d_real,\n",
    "            },'Discriminator/model_{}.pth'.format(epoch_dis+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(rl_d_fake)),rl_d_fake,color='blue')\n",
    "plt.plot(range(len(rl_d_real)),rl_d_real,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
